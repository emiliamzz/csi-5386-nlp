{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "Train a simple classifier, as a baseline. It could be a traditional classifier (SVM, Random Forest, NB, or other), or using some pre-trained models based on deep learning (pre-trained word embeddings or text embeddings or other models, fine-tuned or not). In fact, there are two baselines provided, based on transformers. You can run at least one of them and explain in your report what method was used and what was the accuracy you obtained."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3beacf20383b5b0c"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/emilia/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package stopwords to /home/emilia/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/emilia/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/emilia/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from collections import defaultdict\n",
    "from nltk.corpus import wordnet as wn\n",
    "from sklearn import svm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-19T00:44:33.763898Z",
     "start_time": "2024-03-19T00:44:33.498212Z"
    }
   },
   "id": "cddac348ee756c78",
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Import data\n",
    "train_data = pd.read_json(path_or_buf='./train/subtaskA_train_monolingual.jsonl', lines=True)\n",
    "test_data = pd.read_json(path_or_buf='./test/subtaskA_monolingual.jsonl', lines=True)\n",
    "gold_data = pd.read_json(path_or_buf='./gold/subtaskA_monolingual.jsonl', lines=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-19T00:44:42.006345Z",
     "start_time": "2024-03-19T00:44:34.691944Z"
    }
   },
   "id": "b91e863279750d65",
   "execution_count": 18
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Preprocess data\n",
    "def preprocess(df):\n",
    "    # Make lowercase\n",
    "    df['text'] = [entry.lower() for entry in df['text']]\n",
    "    # Tokenize words\n",
    "    df['text'] = [word_tokenize(entry) for entry in df['text']]\n",
    "    # Remove stop words and stem/lemmatize\n",
    "    tag_map = defaultdict(lambda : wn.NOUN)\n",
    "    tag_map['J'] = wn.ADJ\n",
    "    tag_map['V'] = wn.VERB\n",
    "    tag_map['R'] = wn.ADV\n",
    "    for index, entry in enumerate(df['text']):\n",
    "        processed = []\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        for word, tag in pos_tag(entry):\n",
    "            if word not in stopwords.words('english') and word.isalpha():\n",
    "                final = lemmatizer.lemmatize(word,tag_map[tag[0]])\n",
    "                processed.append(final)\n",
    "        df.loc[index,'text_final'] = str(processed)\n",
    "    df.drop('text', axis=1)\n",
    "    return df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-19T00:44:42.024390Z",
     "start_time": "2024-03-19T00:44:42.011219Z"
    }
   },
   "id": "c5c04b0268fece5c",
   "execution_count": 19
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "train_preprocessed = preprocess(train_data)\n",
    "gold_preprocessed = preprocess(gold_data)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2024-03-19T00:44:42.027039Z"
    }
   },
   "id": "10e5207bfff052f6",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "train_preprocessed_file = open('train.p', 'wb')\n",
    "pickle.dump(train_preprocessed, train_preprocessed_file)\n",
    "train_preprocessed_file.close()\n",
    "\n",
    "gold_preprocessed_file = open('gold.p', 'wb')\n",
    "pickle.dump(gold_preprocessed, gold_preprocessed_file)\n",
    "gold_preprocessed_file.close()"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "c477cb348d78f0ca"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "X_train = train_preprocessed.drop('label', axis=1)\n",
    "y_train = train_preprocessed['label']\n",
    "X_gold = gold_preprocessed.drop('label', axis=1)\n",
    "y_gold = gold_preprocessed['label']"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "7e4c5397f71f6639",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Vectorize using TF-IDF\n",
    "vectorizer = TfidfVectorizer(max_features=5000)\n",
    "vectorizer.fit(X_train['text_final'])\n",
    "train_X = vectorizer.transform(X_train)\n",
    "\n",
    "vectorizer.fit(X_gold['text_final'])\n",
    "gold_X = vectorizer.transform(X_gold)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "ef5a79cfe61f9918"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Train baseline and get accuracy based on gold standard\n",
    "baseline = svm.SVC()\n",
    "baseline.fit(train_X, y_train)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "2e63960d36bd98df",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Calculate accuracy\n",
    "y_pred = baseline.predict(gold_X)\n",
    "print(accuracy_score(y_pred, y_gold))"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "2d2868c6df1b73fa"
  },
  {
   "cell_type": "markdown",
   "source": [
    "    Train at least two advanced classifiers based on deep learning, such as fine-tuning a type of BERT model for the first method (though not the version from the baseline in part 1);  and using a recent type of generative LLM for the second method (such as Llama or something equivalent).  Use part of the training data for validation (or use the dev data for validation) when building your models and keep aside the test data for the final testing. (Alternatively, you can try prompt-based learning with LLMs for the second method)."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fa535a15fc81cbee"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "847eb89f5f639f71"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
